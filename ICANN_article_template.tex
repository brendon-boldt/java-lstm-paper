
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage[sort,numbers]{natbib}
\usepackage{tikz,pgfplots}

\usepackage{url}
\urldef{\mailsa}\path{brendon.boldt1@marist.edu}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Using LSTMs to Model the Java Programming Language}

% a short form should be given in case it is too long for the running head
\titlerunning{Using LSTMs to Model the Java Programming Language}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Brendon Boldt}
%
\authorrunning{Brendon Boldt}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Marist College,\\
3399 North Rd. Poughkeepsie, NY, USA\\
\mailsa\\
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle

\begin{abstract} 
Recurrent neural networks (RNNs), specifically long-short term memory 
networks (LSTMs), can model natural language effectively. This research 
investigates the ability for these same LSTMs to perform next ``word'' 
prediction on the Java programming 
language. Java source code from four different repositories
undergoes a transformation that preserves the logical structure of the 
source code and removes the code's various specificities such as 
variable names and literal values. Such datasets and an additional English 
language corpora are used to train and test standard LSTMs' ability to predict
the next element in the sequence. 
Results suggest that LSTMs can effectively model
Java code achieving perplexities under $22$ and accuracies above $0.47$, which
is an improvement over LSTMs over the English language which achieves a 
perplexity of $85$ and an accuracy of $0.27$. This research can have further
meaning in other areas such as syntactic template suggestion and automated bug
patching.
\end{abstract} 

\section{Introduction}
\label{submission}

Machine learning techniques of language modeling are often applied
to natural languages, but techniques used to model natural languages
such as $n$-gram, graphed-based, and context sensitive models
can be applicable to programming languages as well
\cite{Allamanis} \cite{Nguyen} \cite{Asaduzzaman}.
One such an application of a language model is next-word prediction
which can prove very useful for tasks such as syntactic template
suggestion and bug patching
\cite{Nguyen} \cite{Kim}.
There has been research into programming language models
which use Bayesian statistical inference ($n$-gram models)
to perform next-word prediction \cite{Allamanis}.
Yet some of the most successful natural language models have been
built using recurrent neural networks (RNNs); their ability to
remember data over long sequences makes them particularly apt for
word prediction \cite{LSTMArticle}.

Specifically, long-short term memory (LSTM) RNNs have further improved
the basic RNN model by increasing the ability of an RNN to remember
data over a longer sequence of input without the signal decaying
quickly \cite{LSTMArticle}. LSTMs are a sequence-to-word language
model which means given a sequence of words (e.g., words in the
beginning of a sentence), the model will produce a probability
distribution describing what the next word in the sequence is.
Equation \ref{seqtoword} illustrates the basic structure of a
sequence-to-word language model where $L$ is the language model,
$w_n$ is the $n$th word in the sequence, and $W_{n+1}$ is a vector
describing the probability distribution describing which word $w_{n+1}$
is.

\begin{equation}
\label{seqtoword}
    L(w_1, w_2, w_3, \dots, w_n) = W_{n+1}
\end{equation}

We are specifically investigating next-statement prediction in method
bodies. While other parts of Java source code (e.g., class fields,
import statements) do have semantic significance, method bodies make up
the functional aspect of source code\footnote{
Functional insofar as method bodies describe that actual
behavior of the program.} and most resemble natural language sentences.
Just as individual semantic tokens (words) comprise natural language
sentences, statements, which can be thought of as semantic tokens,
comprise method bodies. Furthermore, the semantics of individual natural
language words coalesce to form the semantics of sentence just as the
semantics of the statement in a method body form the semantics of the
method as a whole. By this analogy, language modeling techniques which
operate on sentences comprised of words could apply similarly to method
bodies comprised of statements.

%~%~% Also \subsubsection{}

%should be given.  See Section~\ref{final author} for details of how to

\section{Tokenizing Java Source Code}

We are specifically looking at predicting the syntactic structure  of next 
statement in within Java source code method bodies. The syntactic structure 
of a complete piece of source code is typically represented in an abstract 
syntax tree (AST) where each node of the tree represents a distinct 
syntactic element (e.g., statement, boolean operator, literal integer). 
Method bodies are, in particular, comprised of statements which, more or 
less, represent a self-contained action. Each of these statements is the 
root of its own sub-AST which represents the syntactic structure of only 
that statement. For this reason, the statements are the smallest 
independent, semantically meaningful unit of a method body and are suitable 
to be tokenized for input into the RNN.

Nguyen et al. \cite{Nguyen} studied a model for syntactic statement prediction 
called ASTLan which uses Bayesian statistical inference to interpret and 
predict statements in the form of sequential statement ASTs. While Bayesian 
statistical inference can be applied to statements directly in their AST 
form, RNNs operate on independent tokens such as English words. Thus, it is 
necessary that statement ASTs be flattened into a tokenized form in order to 
produce an RNN-based model.

\subsection{Statement-Level AST Tokenization}

The RNN model described in \citet{LSTMArticle} specifically uses space-delimited 
text strings, hence, when the statement ASTs are tokenized, they 
must be represented as space-delimited text strings.

To show the tokenization of Java source, take the following Java statement:

\begin{verbatim}
    int x = obj.getInt();
\end{verbatim}

The corresponding AST, as given by the Eclipse AST parser, appears in Figure \ref{ast-figure} \cite{Eclipse}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=70mm]{ast.eps}}
\caption{The abstract syntax tree (AST) representation of of the Java statement
\texttt{int x = obj.getInt();}}
\label{ast-figure}
\end{center}
\vskip -0.2in
\end{figure} 

This statement, in turn, would be transformed into the following token\footnote{
\texttt{VariableDeclarationStatement} is not included in the
tokenized version of the AST since the syntax is adequately represented
by starting with the root node's children.}:

%~%~% Say why `VariableDeclarationStatement` does not appear

%\begin{addmargin}[0.5in]{0in}
\begin{verbatim}
    _PrimitiveType_VariableDeclarationFr
    agment(_SimpleName_MethodInvocation(_
    SimpleName_SimpleName)))

    _60(_39_59(_42_32(_42_42)))
\end{verbatim}
%\end{addmargin}


\iffalse This could be a reference table is needed.
\begin{table}[t]
    \caption{Total size of each corpora measured in words. The approximate
    split between training, validation, and test data is $80\%$, $10\%$,
    and $10\%$ respectively.}
    \label{size-table}
    \vskip 0.15in
    \begin{center}
    %\begin{small}
    \begin{tabular}{lr}
    \hline
    Corpus & \multicolumn{1}{c}{Size} \\
    \hline
    PTB                 & $1085779$ \\
    JDK                 & $303560$ \\
    Guava               & $259686$ \\
    ElasticSearch       & $561697$ \\
    Spring Framework    & $526968$ \\
    \hline
    \end{tabular}
    %\end{small}
    \end{center}
    \vskip -0.1in
\end{table}
\fi

In the representation used for the LSTM, the AST node names are replaced with
integer IDs corresponding to each type of AST node
(e.g., $60$ corresponds to ``PrimitiveType'' nodes and $42$ corresponds to
``SimpleName'' nodes).
We have included the named version to demonstrate
how it fits in with the visual AST.  Individual AST nodes are
separated by underscores (``\texttt{\_}'') and parentheses are used
to denote a parent-child relationship so that the tree structure of
the statement is preserved. In fact, it is possible  to recreate the
syntax of the original source  code from the the tokens; thus, this 
tokenization is lossless in terms of  \textit{syntactical} information 
yet lossy in other areas. For example, variable and function names are
discarded during the translation to make the model independent of
variable and function names.


\subsection{Method-Level Tokenization}

Now let us consider the following;

%\begin{addmargin}[0.5in]{0in}
\begin{verbatim}
    int foo() {
        int x = obj.getInt();
        if (x > 0) {
            x = x + 5;
        }
        return x;
    }
\end{verbatim}
%\end{addmargin}

Each statement in the method body is tokenized just as the single statement 
was above where the tokens are space delimited. Braces, while not 
statements, are included (denoted by ``\texttt\{'' and
``\texttt\}''  to retain the semantic structure of the method body. 
Note that the return type and parameters are included as the first 
token with a leading ``\texttt('' to denote that it is a method
signature (no other statement tokens begin with an open paren).
The method above becomes the following sequence of tokens:
%Just as before, the named and the numbered  version are displayed for clarity.

%\begin{addmargin}[0.5in]{0in}
\begin{verbatim}
    (_39_42 { _60(_39_59(_42_32(_42_42)))
     _25(_27(_42_34) { _21(_7(_42_27(_42_
     34))) } _41(_42) } 
\end{verbatim}
%\end{addmargin}

%~%~% I will only include these data sets if I have time.
\iffalse
In addition to representing the only the syntactic structure of the AST, tokens 
can also be generated to include the following information: the name of the 
type specified (e.g., in the case of PrimitiveType or ReferenceType), the 
specific operator (e.g., in the case of InfixExpression, PrefixExpression, 
etc.), and certain frequently occurring constants (e.g., \texttt 0,
\texttt 1, \texttt{true}, \texttt{false}, 
et&c.).

%\begin{addmargin}[0.5in]{0in}
\begin{verbatim}
    (_39:int_42 { _60(_39:int_59(_42_32(_
    42_42))) _25(_27:>(_42_34:0) { _21(_7
    (_42_27:+(_42_34))) } _41(_42) }
\end{verbatim}
%\end{addmargin}
\fi

The space-delimited sequence of these tokens forms a ``sentence'' which  
directly  correlates to the body of a single Java method. These individual  
tokens will then comprise the vocabulary that the LSTM network uses to train 
and make predictions.

\subsection{English and Java Source Corpora Used}

Similarly to \citet{LSTMArticle}, we are using the Penn Treebank (PTB) for the
English language corpus as it provides an effective, general sample of the English
language. %~%~% Back this up?
For the Java programming languages, four different corpora were built from
the source code of projects (one project is built into one corpus). The
Java Development Kit (JDK), Google Guava, ElasticSearch, and Spring Framework.
The JDK is a good reference for Java since it is largest implementation
of the Java language; the other three projects were selected based on their
high popularity on GitHub in addition to the fact they are
Java-based projects.

It is important to note that the
PTB does not contain any punctuation while the tokenized
Java source contains ``punctuation'' only in the form of statement
body-delimiting curly braces (``\texttt\{'' and ``\texttt\}'')
since these are integral to the semantic structure of source code.

% BB: Is the table explanation sufficient?

While the PTB data set is larger than the Java corpora, all datasets
are within one order of magnitude in size. The exact sizes of the
data sets are given by Table \ref{size-table}.

\begin{table}[t]
    \caption{Total size of each corpora measured in words. The approximate
    split between training, validation, and test data is $80\%$, $10\%$,
    and $10\%$ respectively.}
    \label{size-table}
    \vskip 0.15in
    \begin{center}
    %\begin{small}
    \begin{tabular}{lr}
    \hline
    Corpus & \multicolumn{1}{c}{Size} \\
    \hline
    PTB                 & $1085779$ \\
    JDK                 & $303560$ \\
    Guava               & $259686$ \\
    ElasticSearch       & $561697$ \\
    Spring Framework    & $526968$ \\
    \hline
    \end{tabular}
    %\end{small}
    \end{center}
    \vskip -0.1in
\end{table}


\subsection{Vocabulary Comparison}

In addition to preserving the logical structure of the source code when
tokenizing it, another goal of the specific method of tokenization was to
produce a vocabulary with a frequency distribution similar to that of
English (compared against the English corpora used, that is). If the same
Java statement tokens appear too frequently, the tokenization might be
generalizing the Java source too much such that it loses the underlying
semantics. If the statement tokens, instead, all have a very low frequency
it would be difficult to effectively perform inference on the sequence of 
tokens within the allotted vocabulary size.

In all of the Java corpora, the left and right curly braces comprise
approximately $35\%$
of the total tokens present. This a disproportionately high number in
comparison to the rest of the tokens, but removing them from the frequency
distribution, since they classify as punctuation, gives a more accurate
representation of the vocabularies. The adjusted frequency distribution
shown in Figure \ref{english-frequency} compares the PTB to the
JDK source code. The rate of occurrence for the highest
ranked words is significantly higher in the JDK than in the PTB, but
the frequency distributions track closely together beyond the fifth-ranked
words.

The statistical similarities between the English and the translated Java
corpora suggest that the Java statement tokens have an adequate amount of
detail in terms of mimicking English words. If the Java statement tokens
were too detailed, their frequencies would be far lower than those of English
words; if the Java statement tokens were not detailed enough, their
frequencies would be much higher than those of English words.

\begin{figure}
\begin{tikzpicture}
	\begin{axis}[
	    height=5cm,
		xlabel=Word Rank,
		ylabel=Frequency,
		xmin=0,
		xmax=30]
		\addlegendentry{PTB}
		\addplot [dashed] table [
	    	x=rank,
		    y=PTB,
		    mark=none,
		    col sep=tab] {freq.dat};
		\addlegendentry{JDK}
		\addplot table [
	    	x=rank,
		    y=JDK,
		    mark=none,
		    col sep=tab] {freq.dat};
	\end{axis}
\end{tikzpicture} 
\caption{Comparison of English and Java word frequency distributions.
    The $y$-axis represents the total proportion of the word with a given
    rank (specified by the $x$-axis).}
\label{english-frequency}
\end{figure}
 
Adjusting the four Java corpora in the same way (removing the left and right
curly braces) yields similar frequency distributions across all word ranks
(see Figure \ref{java-frequency}).
 
\begin{figure}
\begin{tikzpicture}
	\begin{axis}[
	    height=5cm,
		xlabel=Word Rank,
		ylabel=Frequency,
		xmin=0,
		xmax=30]
		\addlegendentry{JDK}
		\addplot table [
	    	x=rank,
		    y=JDK,
		    mark=none,
		    col sep=tab] {freq.dat};
		\addlegendentry{Guava}
		\addplot table [
	    	x=rank,
		    y=Guava,
		    mark=none,
		    col sep=tab] {freq.dat};
		\addlegendentry{ElasticSearch}
		\addplot table [
	    	x=rank,
		    y=ElasticSearch,
		    mark=none,
		    col sep=tab] {freq.dat};
		\addlegendentry{Spring Framework}
		\addplot table [
	    	x=rank,
		    y=SpringFramework,
		    mark=none,
		    col sep=tab] {freq.dat};
	\end{axis}
\end{tikzpicture} 
\caption{Comparison of Java corpora frequency distributions}
\label{java-frequency}
\end{figure}

\iffalse
    Another consideration when comparing the English and Java corpora are the
    prevalence of the metatokens \texttt{<eos>} denoting the end of a
    sentence and \texttt{<unk>} denoting a token not contained in the vocabulary.
\fi


%~%~% Should I use the adjusted metrics here?
\begin{table}[t]
    \caption{Proportion and rank of the metatoken%s \texttt{<eos>} and
    \texttt{<unk>}. Proportions and ranks are from the adjusted Java corpora
    with the left and right curly braces removed.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
    %\begin{small}
    %\begin{tabular}{lcccc}
    \begin{tabular}{lcc}
    \hline
         %& \multicolumn{2}{c}{\texttt{<eos>}} &
                %\multicolumn{2}{c}{\texttt{<unk>}} \\
        %Corpus & Prop. & Rk. & Prop. & Rk. \\
        %\hline
        %\abovespace
        %PTB                 & $0.0453$ & $3$ & $0.0484$ & $2$ \\
        %JDK                 & $0.2664$ & $1$ & $0.0724$ & $2$ \\
        %Guava               & $0.3245$ & $1$ & $0.0476$ & $5$ \\
        %ElasticSearch       & $0.2641$ & $1$ & $0.1618$ & $2$ \\
        %Spring Framework    & $0.3203$ & $1$ & $0.0873$ & $2$  \\
    Corpus & Proportion & Rank\\
    \hline
    PTB                 & $0.0484$ & $2$ \\
    JDK                 & $0.0724$ & $2$ \\
    Guava               & $0.0476$ & $5$ \\
    ElasticSearch       & $0.1618$ & $2$ \\
    Spring Framework    & $0.0873$ & $2$  \\
    \hline
    \end{tabular}
    %\end{small}
    \end{center}
    \vskip -0.1in
\end{table}

Another consideration when comparing the English and Java corpora is the
prevalence of the metatoken \texttt{unk} which denotes a token not contained
in the language model's vocabulary.
Due to the nature of LSTMs, the vocabulary of the language model is finite;
hence, any word not contained in the vocabulary is considered unknown.
We specifically used a vocabulary size of $10,000$. A vocabulary size which
is too small will fail to represent enough words in the corpus; the result
is the LSTM seeing a high proportion of the \texttt{unk} metatoken. A
vocabulary which is too large increases the computation required during
training and inference.
The proportion of \texttt{<unk>} tokens in both the English and the Java
source data sets (save for ElasticSearch) are $<10\%$ which indicates that
the $10,000$ word vocabulary accounts for approximately $90\%$ of the corpus'
words by volume. It is important that the Java corpora's \texttt{<unk>}
proportion is not significantly higher than that of the PTB since
that would suggest that $10,000$ is too small a vocabulary size to describe
the tokenized Java source code.

\iffalse
It is the case, though, that all of the Java copora show much higher
\texttt{<eos>}. This indicates that the sentences of the Java copora have
a lower average word count.
%~%~% Does this affect anything?
\fi

\section{Language Modeling}
\label{language-modeling}

\subsection{Neural Network Structure and Configuration}

In order to make a good comparison between language modeling in English
and Java, a model with demonstrated success at modeling English was
chosen. The model selected was a LSTM neural
network, a type of RNN, as described in
\citet{LSTMArticle}. This particular LSTM uses regularization via 
dropout to act as a good language model for natural languages
such as English \cite{LSTMArticle}.
%when measuring word level perplexity 

% BB: Talk about training size at all?

The LSTM's specific configuration was the same as the ``medium''
configuration described in \citet{LSTMArticle} with the exception
that the data was trained for $15$ epochs instead of $39$ epochs.
Beyond $15$ epochs (on both the English and Java datasets), the 
training cost metric (perplexity) continued to decrease while the
validation cost metric remained steady. This suggests that the model
was beginning to overfit the training data and that further training
would not improve performance on the test data.
sets tested. Notably, this model contains two RNN layers with a vocabulary
size of $10,000$ words.

% BB: Do I need to be more specific?

Each corpora was split into partitions such that $80\%$ was training data
and the remaining $20\%$ was split evenly between test and validation
data. Perplexity, the performance metric of the LSTM, is determined by the
ability of the LSTM to perform sequence-to-word prediction on the test
set of that corpus. Perplexity represents how well the prediction (in the
form a probability distribution) given by the LSTM matches the actual
word which comes next in the sentence. A low perplexity means that the
language model's predicted probability distribution matched closely the
actual probability distribution, that is, it was better able to predict
the next word. Perplexity is the same metric that is used in
\citet{LSTMArticle} to compare language models.

\subsection{Language Model Metrics}

We chose word-level perplexity was chosen as the metric for comparing the
language models' performance on the given corpora since it provides
a good measurement of the models overall ability to predict words
in the given corpus \cite{sundermeyer2015feedforward}. 
Perplexity for a given model is calculated
by exponentiating (base $e$) the mean
cross-entropy across all words in the test set. This is formally
expressed as follows:

\begin{equation}
\label{perplexity}
    P(L) = \exp\left(\frac{1}{N}\sum^{N}_{i=1} H(L,w_i)\right)
\end{equation}

where $N$ is the test data set size, $L$ is the language model, $w_i$
is the $i$th word in the test set, and $H(L, w_i)$ is the natural log
cross-entropy from $w_i$ to the prediction given by $L(w_i)$. A lower
perplexity represents a language model with better predictive
performance \cite{wang2016parallel}.

The cross-entropy is the opposite of summing the
product of the probability of that word appearing, i.e., $1$ for the
correct word and $0$ for all other incorrect words, and the
natural logarithm of the output value of LSTM's softmax layer.
The cross-entropy is defined as follows:

\begin{equation}
\label{cross-entropy}
    H(L,w) = - \sum_{i=1}^V p(w_i) \ln L(w_i)
\end{equation}

where $V$ is the vocabulary size and $p(w_i)$ is the probability of
$w_i$ being the correct word. Since the probability for incorrect
words is $0$ and the correct word is $1$, the sum can be reduced to
$1$ times the the natural log of the probability of the correct word
as given by the LSTM.

\begin{equation}
    H(L,w) = - \ln L_w(w)
\end{equation}

where $L_w(w)$ is the LSTM's softmax output specifically for the
word $w$. Additionally, the word-level accuracy was calculated for
each language model considering the top $1$, $5$, and $10$ predictions
made by the model.

% PR: Is there any way you could refer to other work's results
%     as a comparison (word?)? You need to discuss the results more.
%     What does it all mean? Why does it work?

% BB
% Maybe? http://www1.icsi.berkeley.edu/Speech/docs/HTKBook3.2/node188_mn.html
% Comparison is not very easy since perplexity is sensitive to various
% changes in the LM used.

% BB: Should I measure the entropy of the corpora?

\section{Results}

\begin{table}[t]
    \caption{Perplexities given by Equation \ref{perplexity}.}
        %Entropy is the Shannon entropy calculated using
        %$\ln(P) / \ln(2)$.}
    \label{result-table}
    \vskip 0.15in
    \begin{center}
    %\begin{small}
    \begin{tabular}{lcl}
    \hline
    Corpus & Perplexity & Language \\
    \hline
    PTB                 & $85.288$ & English \\
    JDK                 & $21.808$ & Java \\
    Guava               & $18.678$ & Java \\
    ElasticSearch       & $11.397$ & Java \\
    Spring Framework    & $11.318$ & Java \\
    \hline
    \end{tabular}
    %\end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[t]
    \caption{Proportion of predictions which had the correct
    word in their top $k$ predictions.
    ``ElasticSearch'' is written as ``ES'' and ``Spring
    Framework'' is written as ``SF''.}
        %Entropy is the Shannon entropy calculated using
        %$\ln(P) / \ln(2)$.}
    \label{topk-table}
    \vskip 0.15in
    \begin{center}
    %\begin{small}
    \begin{tabular}{lcccl}
    \hline
    Corpus & Top 1 & Top 5 & Top 10 & Language \\
    \hline
    PTB                 & $0.269$ & $0.470$ & $0.552$ & English \\
    JDK                 & $0.474$ & $0.652$ & $0.716$ & Java \\
    Guava               & $0.519$ & $0.696$ & $0.751$ & Java \\
    ES       & $0.576$ & $0.739$ & $0.784$ & Java \\
    SF    & $0.560$ & $0.722$ & $0.783$ & Java \\
    \hline
    \end{tabular}
    %\end{small}
    \end{center}
    \vskip -0.1in
\end{table}

The the perplexities achieved on the copora by the LSTM are displayed in
Table \ref{result-table}. The smallest perplexity for non-English data sets 
was measured for the Spring Framework, while the largest was for the JDK data. 
The table also indicates that all four Java data sets showed a drastic 
reduction in perplexity compared to the English data set. Nonetheless, the 
perplexity achieved on the English dataset is similar to that reported by 
\citet{LSTMArticle}. These results indicate the superiority of LSTMs on both
programming languages and a language as complex as the English language.

Table \ref{topk-table} shows the top-$k$ accuracies for each copora.
Clearly, results suggest that the proposed LSTM model is able to more
accurately model pre-processed Java source code than it can English. The table
also indicates that, for the English data set, the use of a large number of
predictors can dramatically increase the overall rate of predictors with the
correct next word; e.g., increasing from one to ten predictors at least doubled
the proportion of predictors. There is a similar effect over Java-based data
sets; however, in these data sets the predictors start at a higher proportion
than with English.

%% Here it could be interesting to insert an actual example of the piece of
%% code that was better predicted among all of them (something line in section
%% 2.2), and also one of the worst predicted of all. If that is available,
%% maybe we can show how things are predicted as you go, word by word.

\section{Conclusion}

In this paper, we have presented a way of modeling a predictive strategy
over the Java programming language using an LSTM. Using datasets such as PTB,
JDK, Guava, ElasticSearch, and Spring Framework we have shown that
LSTMs are suitable in predicting the next syntactic statements of source
code based on preceding statements. Results indicate that the indicate
that LSTMs can achieve lower perplexities and, hence, produce more accurate models
on the Java datasets than the English dataset.

The pre-processed Java code represents a very general and cursory
representation of the original code as it does not include anything such
as variable names or variable types. Future research along these lines
could account for information such as variable types, literal values,
operator values, etc. Additionally, other machine learning methods like
a naive Bayesian classifier could be paired with the LSTM to predict
variable names as well as the syntactic structure of the next statement.
It would also be beneficial to compare the modeling of Java with other
programming languages or to train the model across multiple repositories
in one language.




%articles , conference publications \cite{langley00}, book chapters \cite{Newell81}, books \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{ieeetr}

\end{document} 



